# Bahavior

* [Dataset research](https://github.com/mxm0312/Generative_models/blob/main/behavior_research.ipynb)

  * При помощи PCA было найдено необходимое минимальное число главных компонент для сохранения `95% информации`. В результате удалось уменьшить признаковоее пространство вредоносного поведения `с 853 до 105`
  * Перед использованием PCA был выбран препроцессинг `MinMaxScaler` для признаков, чтобоы отнормировать их.
  * Результаты из этого ноутбука отражены в классе [BehaviourDataset](https://github.com/mxm0312/Generative_models/blob/main/datasets/behavior.py)
  
 * [Bahavior Classifier](https://github.com/mxm0312/Generative_models/blob/main/bahavior_classifier.ipynb)
    * Исходный датасет был поделен на две части: train и val. Первая использовалась для обучения классификатора, а вторая соответственно для валидации на новых данных
    * Наиболее удачной оказалась архитектура полносвязной нейронной сети. На ней удалось достичь 81% точности на валидации. Далее функция потерь и точность валидации выходит на плато
    
  # Generative models research
  
  * [AE, VAE on MNIST](https://github.com/mxm0312/Generative_models/blob/main/AE_VAE_MNIST.ipynb)
    * Были исследованы базовые типы генеративных сетей и выявлены их слабые и сильные стороны.
    * AE - самая простая архитектура генеративной сети, главным минусом является разреженность латентного пространства
    * VAE - архитектура, которая обучает наше латентное пространство под определенное вероятностное распределение, а именно распределение с нулевым средним и единичной дисперсией. В нем пространво становится непрерывным и очень плотным, что позволяет просто генерировать новые данные, подавая на вход дискриминатору случайную величину из известного распределения
    
